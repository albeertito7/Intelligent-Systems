Adversarial Search
	
- decide how to act when there's an adversary/agent in your world
- you're not the only person action in the environment

We examine the problems that arise when we try to plan ahead in a world where other agents are planning against us

Multiagent environments:
	- each agent needs to consider the actions of other agents and how they affect its own welfare
	- the unpredictability of these other agents introduce contingencies, to act against the adversarials

GOAL:
	- to have a CONTINGENT PLAN to win the game, to neutralize the adversarial agents
	- we need to give a function which tells us in any given state what to do


We look into competitive environments, where agentsâ€™ goals are in conflict, giving rise to ADVERSARIAL GAMES


**Reflex agent: just look at ur possible actions and apply a functions about how good would be the actions but just think one step ahead, not building a tree as the minimax function**
**Reflex agent is not optimal and not exponential**


Most common games are: deterministic, turn-taking, two-player, zero-sum games of perfect information (e.g. chess)
    - Deterministic: you know the result of the actions
    - Turn-taking: game based on turns among players
    - Zero-sum: every gain of one player comes because the other is losing, balanced
    - Perfect information: we know every thing about the game, about the state of the game
    - Imperfect information: we don't know everything, a partial view (e.g. shooter: only is known our game view)
    - Chance: you don't know exactly the result of the actions, there is involved the probability


How to well describe a game formally? And then we will be able to apply algorithms

A deterministic game can be formally defined as a kind of search problem with the following elements:
    - S0: initial state: how the game is set up at the start
    - player(s): define which player has the move in a state s
    - actions(s): possible moves for a state, returns the set of legal moves in a state s
    - result(s, a): transition model, given a state and an action which is the result state
    - terminal-test(s): Is the game over? a test which is true when the game in a state is over or otherwise is false
    - utility(s, p):  
        - A utility function, defines the final numeric value for a game that ends in terminal state s for a player p
	- tells us for an end state s how much this is worth for a player p
        - In chess, outcome is a win, loss, or draw, with values +1, 0, or 1
        - defines the game result for the player p in a terminal state s
        - evaluation function; p = player
	- how good is the sate for the specified player


**The solution for a player is a policy which maps State to Actions**
**The initial state, ACTIONS function, and RESULT function define the game tree, where nodes are game states and the edges are moves**


Zero-Sum Games:
	- agents have opposite utilities
	- pure competition -> one function where one agent maximizes it the other agent minimizes it

General Games:
	- agent have independent utilities


Value Concept:
	- is the best achievable outcome (utility) from that state
	- for terminal states the value is known, given in the definition of the game
	- the max value of a state is defined to be the maximum of the value of its children

Minimax Value:
    - the best outcoume achievable under perfect play against an optimal adversary
    - needs to incorporate the idea of adversarial reasoning
    - is still known what's at the bottom/leafs of the tree because the game defines the terminal states
    - the adversary will try to minimize our utility function
    - the max player must maximize the adversarial possible values
    - computed recursively

Minimax Search:
    - a state-space search tree
    - turn-taking game
    - Compute each node's minimax value: the best achievable utility against a rational (optimal) adversary

Two Player Game (Max-Min) 
    - Max moves first
    - turn-taking untill the game is over
    - zero-sum game: the points go to winner and penalties to loser


Minimax Implementation:

    def max-value(state):
        if terminal state: return state's utility
        initialize v = -infinity
        for each sucessor of state:
            v = max(v, min-value(sucessor))
        return v
    
    def min-value(state):
        if terminal state: return state's utility
        initialize v= +infinity
        for each sucessor of state:
            v = min(v, max-value(sucessor))
        return v

    
MultiAgentMinimaxSearch:

    def value(state):
        if the next agent is MAX: return max-value(state)
        if the next agetn is MIN: return min-value(state)

    def max-value(state):
        if terminal state: return state's utility
        initialize v= -infinity
        for each successor of state:
            v = max(v, value(sucessor))
        return v
    
    def min-value(state):
        if terminal state: return state's utility
        initialize v= +infinity
        for each sucessor of state:
            v = min(v, value(sucessor))
        return v


Minimax Efficiency:
    - problem: the number of states to explore could be exponential, like DFS or BFS
    - where r is the branching factor and m is the depth level to be reached:
        - time: O(r^m) -> on each level the time will be more because of the exponential factor 
        - space: O(rm) -> the max possible space to have in our stack


Minimax Properties:
    - Against an optimal player -> Max must play optimally
    - But, otherwise? random or bad player
        - how minimax acts reasoning can lead to suboptimal adversarial behaviour? -> minimax is going to be overly pessimistic


Resource Limits:
    - problem: in realistic games, cannot search to leaves! -> too much memory and computation required
    - solution: Depth-limited search using an evaluation function for non-terminal positions
    - till now is used the terminal states utilities, defined by the game, to calc the mid state ones
    - when adding the depth concept, our depthest states mostly won't be terminal so is needed a way to calc its utility not having the terminal state reference
    - thus, the evaluation function is born -> value estimate about what the terminal utility would be under minimax play
    - guarantee of optimal play is gone -> all depends on how good is the evaluation function estimate value, but always will be imperfect
    - "the deeper in the tree the evaluation function is less important" -> tradeoff between complexity of features and complexity of computation

Evaluation Function:
    - idea: score non-terminal states in depth-limited search
    - ideal funciton: returns the actual minimax value of the position
    - in practice: came up with a function which on average is positive/negative when the minimax value is positive/negative
    - a game state will be better depending on its features -> have two queens in chess i.e.
    - feature: function which look at the state and return some indicator
    - weight: importance coeficien
    - Eval(s) = w1*f1(s) + w2*f2(s) + ... + wn*fn(s)
    - returned value: the result of a weighted linear sum of state features


Game Tree Pruning:
    - prune: to look for branches that do not to be analysed
    - prune those branches/nodes when is know that are already worst than the candidate


Alpha-Beta Pruning Implementation:
    - alpha: MAX's best option on path to root
    - betha: MIN's best option on path to root

    def max-value(state, alpha, betha):
        initialize v = -infinity
        if terminal state: return state's utility
        for each sucessor of state:
            v = max(v, value(sucessor, alpha, betha))
            if v >= betha: return v
            alpha = max(alpha, v)
        return v

    def min-value(state, alpha, betha):
        initialize v = +infinity
        if terminal state: return state's utility
        for each sucessor of state:
            v = min(v, value(sucessor, alpha, betha))
            if v <= alpha return v
            betha = min(betha, v)
        return v


- there is a problem on the above implementation:
    - sometimes children of the root may have the wrong value
    - when prunning by alpha an betha, should not be prunned when the value is equal to alpha or betha


Alpha-Betha Pruning Efficiency:
    - possible to go deeper into the tree -> branching factor is reduced
    - with "perfect ordering":
        - could double solvable depth
        - where r is the branching factor and m is the depth level to be reached:
            - time complexity drops from O(r^m) to O(r^m/2)


OPTIMAL DECISION IN GAMES

- In a normal search problem, the optimal solution is a sequence of actions leading to a goal state (terminal state that is a win).
- In adversarial search, an optimal solution for MAX is a tree of actions specifying what to do depending of what MIN plays => CONTINGENT PLAN

Min player: the one its trying to take an action to minimize the utility function => the adversarial
Max player: via utility functions know how which possible moves will have to take the min player

Given a game tree, the optimal strategy can be determined from the MINIMAX VALUE of each node, which we write as MINIMAX(n).
**in a tree: nodes are states, edges are actions**

The minimax value of a node is the utility (for MAX) of being in the corresponding state

Obiously, the minimax value of a terminal state is just its utility

**Furthermore, if playing optimally, MAX preferes to move to a state of a maximum value whereas MIN prefers a state of minimum value**

**If MIN does not play optimally, MAX using the minimax algorithm will do even better**

Other strategies against suboptimal opponents may do better than the minimax strategy, but these strategies necessarily do worse against optimal opponents.

THE MINIMAX RECURSIVE ALGORITHM
    - use the second approach => recursive version
    - solved the problem about exponential memory required with the Bredth First Search Minimax algorithm