Markov Decision Processes
    - a way to formalize the idea of non-deterministic search

Non-Deterministic Search
    - when action outcomes are uncertain
    - the outcome isn't entirely under our control

Grid World
    - Maze-like problem
        - the agent lives in a grid
        - the walls block the agent's path
    - Noisy movement
        - actions do not always go as planned
    - the agent receives rewards each time step
        - small "living" reward each step (can be negative)
        - big rewards come at the end (good or bad)
    - two kind of rewards
        1. terminal utilities = big rewards
        2. each step
            - positive: walking is happier
            - negative: will cost pain at every step and things must end quickly
    - Goal:
        - Maximize the sum of rewards

Deterministic Grid World
    - action North -> outcome = North

Non-Deterministic/Stochastic Grid World
    - action North -> outcome?
        - 80% North
        - 10% West
        - 10% East
    - action affected by the ENVIRONMENT or other factors

**When planning we must take into account all the possible outcomes, and if they are worth it**


A MDP is defined by:
    - a set of states s ∈ S
    - a set of actions a ∈ A
    - a transition function T(s, a, s') when s' is a possible result
        - this functions tells how likely s' is
        - conditional probability -> P(s'|s,a)
        - also called the model or the dynamics -> representes how the world evolve and response your actions
    - a reward function R(s, a, s')
        - R(s'|s,a)
        - sometimes just R(s), or R(s')
    - a start state s
    - maybe a terminal state
        - but MDP's may often go forever

MDP's are non-deterministic search problems
    - one way to solve them is with Expectimax Search
    - but there's a better way


What is Markov about MDPs?
    - Markov meaning: given the present state, the future and the past are independent
    - For Markov decision processes, "Markov" means that the probability distribution over the outcomes depends only on the current state and action
    - just like search, where the sucessor funciton could only depend on the current state (not the history)

**For MDP problem, make sure to define the transition function and your state such a wat that the transition probabilities depend only on the current state and an action**


Policies

- in deterministic single-agent search problems -> we wanted an optimal plan, or sequence of actions, form start to a goal

- for MDPs, we want an optimal policy π*: S -> A
    - a policy π gives an action for each state
    - a mapping from States to Actions
    - an optimal policy is one that maximizes expected utility if followed
    - an explicit policy defines a reflex agent

- expectimax didn't compute entire policies
    - it computed the action for a single state only
    - for a given state it did a forward thinking computation that produce one entry of the policy
        - and wherever you landed, expectimax is ran again
    - expectimax is a way of solving this problems, but it doesn't comput and explicit policy
        - bad: you might have to redo a lot of work if you keep ending up in the same states
        - good: because if you have so many state you couldn't write down an explicit policy anyway


Policies and Living Penality
    - depending on the living penalty the agent's policies must be more CONSERVATIVE or not
    - if the penalty is little, the agent may remain in the grid for a long time
    - but if the penalty is higher enough, would be interesting to take some risk or even die quickly to not lose more points


MDP Search Trees
    - s is a state
    - (s, a) is a q-state
    - q-state: commitment of taking an action a in a state s
    - from the q-state it is applied the transitions model T(s,a,s') = P(s'|s,a)
    - from a q-state there is a probability distribution of what might happen
    - **the probabilities are given by the transition function**

Discounting
    - it is reasonable to maximize the sum of rewards
    - it is also reasonable to prefer rewards now to rewards later
    - One Solution: values of reward decay exponentially

How to discount?
    - each time we descend a level, we multiply in the discount once

Wy discount?
    - sooner rewards probably do have higher utility than later rewards
    - also helps our algorithms coverage


Stationary Preferences
    - preferences are stationary if whenever i prefer one sequence to another, if i stick the same reward in front of both, my preferences should be unchanged
    - if i like a better than b now, i should like it better shift it into the future as well
    - [a1, a2, ...] > [b1, b2, ...] <=> [r, a1, a2, ...] > [r, b1, b2, ...]
    - Then: there are only two ways to degine utilities:
        - Additive utility: U([r0, r1, r2, ...]) = r0 + r1 + r2 + ...
        - Discounted utility: U([r0, r1, r2, ...]) = r0 + λr1 + λ^2r2 + ...


**Preferences might not be stationary if certain things change at certain times**


Infinite Utilities?!
    - Problem: What if the game lasts forever?
        - that will be not possible to decide between faster or slow because at infinite time we will get the same results
    - Do we get infinite rewards?
    - Solutions:
        - Finite horizon: (similar to depth-limited search)
            - as the game ends we don't have the problem of comparing infinites
            - terminate episodes after a fixed T steps
            - gives place to nonstationary policies (π depends on time left), because that what you actually do may depend on the amount time left
        - Discounting: use 0 < λ < 1
            - even if there's an infinite sequence of positive rewards then there's this exponentially decaying thing that's going to make sure there's some converges
            - U([r0, ... r∞]) = Σ from t=0 to ∞ of ((λ^t) * rt) <= Rmax/(1 - λ) 
            - smaller λ means smaller "horizon"
            - Idea of Horizon of the agent
                - When things are steeply discounted we care more about achieving rewards right now
                - When things are not steeply discounted we don't care so much about how soon rewards are and we try more for big rewards even if they're later
        - Absorbing state:
            - guarantee that for every policy, a terminal state will eventually be reached
            - as time goes on the probability of achieving a terminal state will be increasing to end as 1, meaning other achieving other states will be less likely as time goes
            - for any sequence of actions done, will be hit a terminal state of probability 1
            - is guaranteed not to have infinite cycles of non terminal states

**In general we'll gonna have discounts to fix this infinty problem**