Markov Decision Processes
    - a way to formalize the idea of non-deterministic search

Non-Deterministic Search
    - when action outcomes are uncertain
    - the outcome isn't entirely under our control

Grid World
    - Maze-like problem
        - the agent lives in a grid
        - the walls block the agent's path
    - Noisy movement
        - actions do not always go as planned
    - the agent receives rewards each time step
        - small "living" reward each step (can be negative)
        - big rewards come at the end (good or bad)
    - two kind of rewards
        1. terminal utilities = big rewards
        2. each step
            - positive: walking is happier
            - negative: will cost pain at every step and things must end quickly
    - Goal:
        - Maximize the sum of rewards

Deterministic Grid World
    - action North -> outcome = North

Non-Deterministic/Stochastic Grid World
    - action North -> outcome?
        - 80% North
        - 10% West
        - 10% East
    - action affected by the ENVIRONMENT or other factors

**When planning we must take into account all the possible outcomes, and if they are worth it**


A MDP is defined by:
    - a set of states s ∈ S
    - a set of actions a ∈ A
    - a transition function T(s, a, s') when s' is a possible result
        - this functions tells how likely s' is
        - conditional probability -> P(s'|s,a)
        - also called the model or the dynamics -> representes how the world evolve and response your actions
    - a reward function R(s, a, s')
        - R(s'|s,a)
        - sometimes just R(s), or R(s')
    - a start state s
    - maybe a terminal state
        - but MDP's may often go forever

MDP's are non-deterministic search problems
    - one way to solve them is with Expectimax Search
    - but there's a better way


What is Markov about MDPs?
    - Markov meaning: given the present state, the future and the past are independent
    - For Markov decision processes, "Markov" means that the probability distribution over the outcomes depends only on the current state and action
    - just like search, where the sucessor funciton could only depend on the current state (not the history)

**For MDP problem, make sure to define the transition function and your state such a wat that the transition probabilities depend only on the current state and an action**


Policies

- in deterministic single-agent search problems -> we wanted an optimal plan, or sequence of actions, form start to a goal

- for MDPs, we want an optimal policy π*: S -> A
    - a policy π gives an action for each state
    - an optimal policy is one that maximizes expected utility if followed
    - an explicit policy defines a reflex agent
    
- expectimax didn't compute entire policies
    - it computed the action for a single state only