Reinforcement Learning

- learn how to act over time based on own experiences

Basic idea:
    - Receive feedback in the form of rewards
    - Agent's utility is defined by the reward function
    - Must (learn to) act so as to maximize expected rewards
    - All learning is based on observed samples of outcomes!

- we have an agent as always
- the agent has actions available, and chose one
- the environment affects the transition between state s and s'
- finally the agent receives back two things: the reward (could be a bonus or a penalty) and the sate s'


- Still assume Markov decision process (MDP)
    - a set of states s
    - a set of actions a (per state)
    - a model T(s,a,s')
    - a reward function R(s,a,s')
- still looking for a policy

- but with a new twist: don't know T or R
    - we don't know which states are good or what the actions do
    - must actually try actions and states out to learn


Offline (MDPs) vs Online (RL)
    - Offline solution: is when you know what actions will do and in computation/simulation in your head you think about consequences, applying a dynamic value iteration or a expectimax search mentally
    - Online learning: if we are in the grid world, we need to jump into the pit because the agent doesn't know it is bad as we assume in the offline solution

** How the agent will be able to learn how to act when is not know what actions do and what rewards are got? **


Model-Based Learning
    - we reduce the reinforcement learning problem to the previous case of a known MDP
    - we will try to figure out the model, what de transitions do and what are the rewards

Model-Based Idea:
    - learn an approximate model based on experience
    - solve for values as if the learned model were correct

    - Step 1: Learn empirical MDP model
        Goal: learn some MDP that we could use to run things like value iteration
        - count outcomes s' for each s,a
        - normalize to give an estimate of T(s,a,s')
        - discover each R(s,a,s') when we experience (s,a,s')
    
    - Step 2: Solve the learned MDP

** Use the samples to reconstruct an approximation of what is unknown **


Unknown P(A): "Model Based"
    - P(a) = num(a)/N
    - E[A] ≈ Σ P(a) · a over all actions a

Unknown P(A): "Model Free"
    - average the samples directly, and do an unweighted average
    - E[A] ≈ 1/N · Σ a over all actions a


Model-Free Learning
    - we don't construct a model of the transition function
    - we take actions, and at each time we take once we compare what we though must happen to what actually happened
    - an wheter something is better or worse of what we expected, we adjust our values
    - So we track the values themselves not the transitions probabilities or the rewards


Passive Reinforcement Learning
    - Idea:
        - some agent is taking actions and getting specific outcomes that are partialy determined by chance
        - we need to learn from what we observe
        - but we don't control the actions, someone else is choosing the actions

    - Input: a fixed policy
    - we don't know the transitions T(s,a,s')
    - we don't know the rewards R(s,a,s')
    - just execute the policy and learn from experience
    - Goal: learn the state values


Direct Evaluation:
    - Goal: compute values for each state under an input fixed policy
    - Idea: Average together observed sample values
        - every time you visit a state, write down what the sum of discounted rewards turned out to be
        - average those samples

    - What's good about direct evaluation?
        - it's easy to understand
        - it doesn't require any knowledge of T,R
        - it eventually computes the correct average values, using just sample transitions

    - What bad about it?
        - it wastes information about state connections -> to get intermediate results weird
        - each state must be learned separately
        - so it takes a long time to learn

    - Why not use policy evaluation?
        - this approach fully exploited the connections between the states
        - unfortunately, we need T and R to do it!
        - key question: How can we do this update to V without knowing T and R?
            - Idea: take samples of outcomes s' (by doing the action!) and average => Sample-Based Policy Evaluation

            - sample1 = R(s,π(s),s1') + λVπ(s1')
            - sample2 = R(s,π(s),s2') + λVπ(s2')
            - sampleN = R(s,π(s),sN') + λVπ(sN')

            - Vπk+1(s) = (1/N) · Σ of all samples

            - but there's a problem: We can't rewind time to get sample after sample from one specific state

Temporal Difference Learning
    - Big Idea: learn from every experience
        - update V(s) each time we experience a transition (s,a,s',r)
        - likely outcomes s' will contribute updates moer often
    
    - temporal difference learning from values
        - policy still fixed, still doing evaluation!
        - move values toward value of whatever successor occurs: running average

        - Sample of V(s): sample = R(s,π(s),s') + λVπ(s')
        - Update to V(s): Vπ(s) = (1 - α) · Vπ(s) + (α) · sample, where α is the learning rate established
            V(s): Vπ(s) = Vπ(s) + α · (sample - Vπ(s))

        - there's a tradeoff:
            - larger alpha -> learn faster
            - smaller alpha -> learn slower -> Decreasing learning rate can give converging averages
        
        - ** where alpha is adjusting the error balancing accross the experiences **


Temporal Difference Learning is doing an Exponential Moving Average
    - the running interpolation update: Xn = (1 - α) · X(n-1) + α · Xn
    - this averaging makes recent samples more important
    - forgets about the past (distant past values were wrong anyway)
    - because of the future value estimation added to the samples, the recent samples are better; so it's amazing this formulation gives less power to the past values


Problems with TD Value Learning
    - is a model-free way to do policy evaluation, mimicking Belmman updates with running sample averages
    - but if we want to turn values into a (new) policy, we are sunk
        - we need to compute Actions from Values and apply the Argmax,
        - which means compute Q-Values from Values, that implies a expectimax over T,
        - but do we have T and R? Nop :(
    - Idea: learn Q-values, not values
    - makes actio selection model-free too -> we look at the Q-values and just choose the one is better

** Q-values are critical for choosing actions in reinforcement learning **


Active Reinforcement Learning
    - you have to try things

    - Full reinforcement learning: optimal policies (like value iteration)
        - we don't know the transitions
        - we don't know the rewards
        - we choose the actions now, not by a fixed policy
        - Goal: learn the optimal policy/values


Detour: Q-Value Iteration

    - Value iteration: find successive (depth-limited) values
        - Start with V0(s) = 0, which we know is right
        - Given Vk, calculate the depth k+1 values for all states
        - Vk+1(s) <- max Σ of all s' (T(s,a,s')[R(s,a,s') + λVk(s')]) over all actions a

    - But Q-Values are more useful, so compute them instead
        - Start with Q0(s,a) = 0, which we know is right
        - Given Qk, calculate the depth k+1 q-values for all q-states
    
    - so:
        Qk+1(s,a) = Σ of all s' (T(s,a,s')[R(s,a,s') + λ · (maxQ(s',a') over all a')]) => sample-based Q-value iteration = Q-Learning


Q-Learning: sample-based Q-value iteration
    - learn Q(s,a) values as we go => we are making q-value approximation and not value approximation
    - we are at Q(s,a), we take the action an we are gonna learn something from this => receive a sample (s,a,s',r)
    - sample = R(s,a,s') + λ · (maxQ(s',a') over all a')
    - finally incorporate the new estimate into a running average:
        - Q(s,a) = (1 - α) · Q(s,a) + (α) · sample

Q-Learning Properties
    - Amazing result: Q-Learning converges to optimal policy --even if you are action suboptimally
    - this is called off-policy learning
    - Caveats:
        - you have to explore enough
        - you have to eventually make the learning rate samll enough ... but not decrease it too quickly
        - basically, in the limit, it doesn't matter how you select actions (!)