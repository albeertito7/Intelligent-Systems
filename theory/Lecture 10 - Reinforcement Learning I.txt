Reinforcement Learning

- learn how to act over time based on own experiences

Basic idea:
    - Receive feedback in the form of rewards
    - Agent's utility is defined by the reward function
    - Must (learn to) act so as to maximize expected rewards
    - All learning is based on observed samples of outcomes!

- we have an agent as always
- the agent has actions available, and chose one
- the environment affects the transition between state s and s'
- finally the agent receives back two things: the reward (could be a bonus or a penalty) and the sate s'


- Still assume Markov decision process (MDP)
    - a set of states s
    - a set of actions a (per state)
    - a model T(s,a,s')
    - a reward function R(s,a,s')
- still looking for a policy

- but with a new twist: don't know T or R
    - we don't know which states are good or what the actions do
    - must actually try actions and states out to learn


Offline (MDPs) vs Online (RL)
    - Offline solution: is when you know what actions will do and in computation/simulation in your head you think about consequences, applying a dynamic value iteration or a expectimax search mentally
    - Online learning: if we are in the grid world, we need to jump into the pit because the agent doesn't know it is bad as we assume in the offline solution

** How the agent will be able to learn how to act when is not know what actions do and what rewards are got? **


Model-Based Learning
    - we reduce the reinforcement learning problem to the previous case of a known MDP
    - we will try to figure out the model, what de transitions do and what are the rewards

Model-Based Idea:
    - learn an approximate model based on experience
    - solve for values as if the learned model were correct

    - Step 1: Learn empirical MDP model
        Goal: learn some MDP that we could use to run things like value iteration
        - count outcomes s' for each s,a
        - normalize to give an estimate of T(s,a,s')
        - discover each R(s,a,s') when we experience (s,a,s')
    
    - Step 2: Solve the learned MDP

** Use the samples to reconstruct an approximation of what is unknown **


Unknown P(A): "Model Based"
    - P(a) = num(a)/N
    - E[A] ≈ Σ P(a) · a over all actions a

Unknown P(A): "Model Free"
    - average the samples directly, and do an unweighted average
    - E[A] ≈ 1/N · Σ a over all actions a


Model-Free Learning
    - we don't construct a model of the transition function
    - we take actions, and at each time we take once we compare what we though must happen to what actully happened
    - an wheter something is better or worse of what we expected, we adjust our values
    - So we track the values themselves not the transitions probabilities or the rewards


Passive Reinforcement Learning
    - Idea:
        - some agent is taking actions and getting specific outcomes that are partialy determined by chance
        - we need to learn from what we observe
        - but we don't control the actions, someone else is choosing the actions

    - Input: a fixed policy
    - we don't know the transitions T(s,a,s')
    - we don't know the rewards R(s,a,s')
    - just execute the policy and learn from experience
    - Goal: learn the state values


Direct Evaluation:
    - Goal: compute values for each state under an input fixed policy
    - Idea: Average together observed sample values
        - every time you visit a state, write down what the sum of discounted rewards turned out to be
        - average those samples

    - What's good about direct evaluation?
        - it's easy to understand
        - it doesn't require any knowledge of T,R
        - it eventually computes the correct average values, using just sample transitions

    - What bad about it?
        - it wastes information about state connections -> to get intermediate results weird
        - each state must be learned separately
        - so it takes a long time to learn

    - Why not use policy evaluation?
        - this approach fully exploited the connections between the states
        - unfortunately, we need T and R to do it!
        - key question: How can we do this update to V without knowing T and R?