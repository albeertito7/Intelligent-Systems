Reinforcement Learning

- learn how to act over time based on own experiences

Basic idea:
    - Receive feedback in the form of rewards
    - Agent's utility is defined by the reward function
    - Must (learn to) act so as to maximize expected rewards
    - All learning is based on observed samples of outcomes!

- we have an agent as always
- the agent has actions available, and chose one
- the environment affects the transition between state s and s'
- finally the agent receives back two things: the reward (could be a bonus or a penalty) and the sate s'


- Still assume Markov decision process (MDP)
    - a set of states s
    - a set of actions a (per state)
    - a model T(s,a,s')
    - a reward function R(s,a,s')
- still looking for a policy

- but with a new twist: don't know T or R
    - we don't know which states are good or what the actions do
    - must actually try actions and states out to learn


Offline (MDPs) vs Online (RL)
    - Offline solution: is when you know what actions will do and in computation/simulation in your head you think about consequences, applying a dynamic value iteration or a expectimax search mentally
    - Online learning: if we are in the grid world, we need to jump into the pit because the agent doesn't know it is bad as we assume in the offline solution

** How the agent will be able to learn how to act when is not know what the actions do and what rewards are got? **