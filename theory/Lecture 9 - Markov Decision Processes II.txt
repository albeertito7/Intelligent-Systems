Markov decision processes II

Recap: MDPs
    - Markov decision processes:
        - states s
        - actions a
        - transitions P(s'| s,a) or T(s,a,s')
        - rewards R(s,a,s') (and discount λ)
        - start state s0
    - Quantities:
        - Policy = map of states s to actions a
        - Utility = sum of discounted rewards
        - Value of a state = what you expected your future utility to be under optimal action/behaviour (max node)
        - Q-Values = expected future utility from a q-state (chance node) (is an average outcome under optimal action)

Optimal Quantities:
    - the value (utility) of a state s:
        - V*(s) = expected utility starting in s and acting optimally
    - the value (utility) of a q-state (s,a):
        - Q*(s,a) = expected utility starting out having taken an action a from state s and (thereafter) acting optimally
    - the optimal policy:
        π*(s) = optimal action from each state s

** Rewards are for one time step, are instanteneous **
** Values are from one point to the end of the game or forever, are accumulative **


The Bellman Equations
    - specify in a recursive way what it means to be optimal or in other words how to compute a value we are interested in
    - How to be optimal:
        - Step 1: take correct first action
        - Step 2: keep being optimal

    - you max when you have a choice between actions and you average when things are not under your control
    - V*(s) = max of Q*(s,a) over all actions a
    - Q*(s) = Σ of all s' (T(s,a,s')[R(s,a,s') + λV*(s')])

    - these are the Bellman equations, and they characterize optimal values in a way we will use over and over


Value Iteration
    - takes the bellman equations and turns them into a method of computing them
    - simply changes the equlity sign of the bellman equation into an assigment
    - Vk+1(s) <- max Σ of all s' (T(s,a,s')[R(s,a,s') + λVk(s')]) over all actions a


Policy Methods

    - Policy evalutation: How good or bad is that policy? How well will i perform if i follow it? For each state what will the value be not under optimal action but under this specific policy?
        - For a specific policy what the values are?

    - Fixed Policies
        - expectimax trees max over all actions to compute the optimal values
        - but if i have a fixed policy i don't have the choice between what action to take, so there's no max over a
        - if we fixed some policy π(s), then the tree would be simplier - only one action per state
            - ... though the tree's value would depend on which policy we fixed
        - so the algorithm to compute will be simplier and faster as well
    
    - Utilities for a Fixed Policy
        - basic operation: compute the utility of a state s under a fixed (generally non-optimal) policy
        - define the utility of a state s, under a fixed policy π:
            Vπ(s) = expected total discounted rewards starting in s and following and following π
        - Vπ(s) = Σ of all s' (T(s,π(s),s')[R(s,π(s),s') + λVπ(s')]) assuming acting not optimally but as π says in the future => Vπ(S')


Why do we evaluate policies?
    - sometimes we will have a policy and wanna know how good it is, just when running value iteration
    - we will see algorithms about starting with a π0 policy, evaluate that one, and look how to improve it


Policy Evaluation
    - How do we calculate the V's for a fixed policy π?
    - Compute Values from Actions
    - Idea 1: turn recursive Bellman equations into updates -> like value iteration
        - Vπ0(s) = 0
        - Vπk+1(s) = Σ of all s' (T(s,π(s),s')[R(s,π(s),s') + λVπk(s')])
        - complexity O(S^2) per iteration
    - Idea 2: without the maxes, the Bellman equations are just a linear system
        - just using any solver of linear equations -> Matlab?


Policy Extraction
    - what happens if we have the values and wanna know what policy we should use depending on them?
    - Computing Actions from Values
    - figure out for the V*(s) which was the action that maps?
    - we need to do a mini-expectimax (one step)
        - π*(s) = Argmax Σ of all s' (T(s,a,s')[R(s,a,s') + λV*(s')]) over all actions a where Argmax is the action that yields the maximum
    - much work, because is needed to reconsider all the actions and all the transitions probabilities
    - requires a one step expectimax for every state to reconstruct


Computing Actions from Q-Values
    - the q-values make super easy to decide between actions
    - π*(s) = Argmax Q*(s,a) over all actions a
    - Important lesson: actions are easier to select from q-values than values!


Problems with Value Iteration
    - Why value iteration is not always the best solution?
    - value iteration repeats/mimics the Bellman updates
    - Problem 1: slow -> O((S^2)A) per iteration
    - Problem 2: The "max" at each state rarely changes -> waste of time and resources; at higher some iterations is needed a big reward sum to change the action established
    - Problem 3: The policy often converges long before the values
    - What can we do? Policy Iteration?

Policy Iteration
    - combines the idea of evaluating one policy (extract Vπ(s)) with the idea of improving that policy from those values
    - alternative approach for optimal values
    - two steps we're gonna alternate:
        - Step 1: Policy evaluation: calculate utilities from some fixed policy (not optimal utilities!) until convergence
        - Step 2: Policy improvement/extraction: update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values
        - Step 3: Repeat this until policy converges
    - It's still optimal!
    - Can converge (much) faster under some conditions -> when there is a larger number of actions but maximizing action doesn't change very much

    - Evaluation: for fixed current policy π, find values with policy evaluation -> iterate until values converge
        - Vπik+1(s) = Σ of all s' (T(s,πi(s),s')[R(s,πi(s),s') + λVπik(s')])
    - Improvement: for fixed values, get a better policy using policy extraction -> one-step look-ahead
        - πi+1(s) = Argmax Σ of all s' (T(s,a,s')[R(s,a,s') + λVπi(s')])

    - we get the seepd-up thanks to the evaluation step


Comparison Value Iteration vs Policy Iteration
    - both compute the same thing -> optimal value for each state
        - we input an MDP and the output is an optimal quantity
        - they both gets optimal values but also provide optimal policy
            - policy iteration provides a policy inmediatly, at it iteration
            - an value iteration we just keep track of it as we go
        
        - In value iteration:
            - every iteration updates both values and (implicitly) the policy
            - we don't track the policy, but taking the max over actions implicitly recomputes it
        - In policy iteration:
            - we do several passses that update utilities with fixed policy (each pass is fast because we consider only one action, not all of them)
            - after the policy is evaluated, a new policy is chosen (slow like a value iteration pass)
            - the new policy will be better (or we're done)


Summary: MDP Algorithms
    - So we want to ...
        - compute optimal values: use value iteration or policy iteration
        - compute values for a particular policy == compute values from actions: use policy evaluation
        - turn your values into a policy == compute actions from values: use policy extraction (one-step look-ahead)
    
    - These all look the same!
        - they basically are - they are all variations of Berllman updates turned into an iteration/recursive algorithm
        - they all use one-step look-ahead expectimax fragments
        - they differ only in whether we plug in a fixed policy or max over actions



Offline Planning Concept:
    - not play the game and only use our knowledge about MDPs to figure out various things such as values of policies, optimal actions and so on
    - just trying to solve a MDP mentally, without doing any computation, just using our knowledge about MDPs
    - but we can only use these offline calculation method when we know the MDP at all