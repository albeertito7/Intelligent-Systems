Markov decision processes II

Recap: MDPs
    - Markov decision processes:
        - states s
        - actions a
        - transitions P(s'| s,a) or T(s,a,s')
        - rewards R(s,a,s') (and discount λ)
        - start state s0
    - Quantities:
        - Policy = map of states s to actions a
        - Utility = sum of discounted rewards
        - Value of a state = what you expected your future utility to be under optimal action/behaviour (max node)
        - Q-Values = expected future utility from a q-state (chance node) (is an average outcome under optimal action)

Optimal Quantities:
    - the value (utility) of a state s:
        - V*(s) = expected utility starting in s and acting optimally
    - the value (utility) of a q-state (s,a):
        - Q*(s,a) = expected utility starting out having taken an action a from state s and (thereafter) acting optimally
    - the optimal policy:
        π*(s) = optimal action from each state s

** Rewards are for one time step, are instanteneous **
** Values are from one point to the end of the game or forever, are accumulative **


The Bellman Equations
    - specify in a recursive way what it means to be optimal or in other words how to compute a value we are interested in
    - How to be optimal:
        - Step 1: take correct first action
        - Step 2: keep being optimal

    - you max when you have a choice between actions and you average when things are not under your control
    - V*(s) = max of Q*(s,a) over all actions a
    - Q*(s) = Σ of all s' (T(s,a,s')[R(s,a,s') + λV*(s')])