Markov decision processes II

Recap: MDPs
    - Markov decision processes:
        - states s
        - actions a
        - transitions P(s'| s,a) or T(s,a,s')
        - rewards R(s,a,s') (and discount λ)
        - start state s0
    - Quantities:
        - Policy = map of states s to actions a
        - Utility = sum of discounted rewards
        - Value of a state = what you expected your future utility to be under optimal action/behaviour (max node)
        - Q-Values = expected future utility from a q-state (chance node) (is an average outcome under optimal action)

Optimal Quantities:
    - the value (utility) of a state s:
        - V*(s) = expected utility starting in s and acting optimally
    - the value (utility) of a q-state (s,a):
        - Q*(s,a) = expected utility starting out having taken an action a from state s and (thereafter) acting optimally
    - the optimal policy:
        π*(s) = optimal action from each state s

** Rewards are for one time step, are instanteneous **
** Values are from one point to the end of the game or forever, are accumulative **


The Bellman Equations
    - specify in a recursive way what it means to be optimal or in other words how to compute a value we are interested in
    - How to be optimal:
        - Step 1: take correct first action
        - Step 2: keep being optimal

    - you max when you have a choice between actions and you average when things are not under your control
    - V*(s) = max of Q*(s,a) over all actions a
    - Q*(s) = Σ of all s' (T(s,a,s')[R(s,a,s') + λV*(s')])

    - these are the Bellman equations, and they characterize optimal values in a way we will use over and over


Value Iteration
    - takes the bellman equations and turns them into a method of computing them
    - simply changes the equlity sign of the bellman equation into an assigment
    - Vk+1(s) <- max Σ of all s' (T(s,a,s')[R(s,a,s') + λVk(s')]) over all actions a


Policy Methods

    - Policy evalutation: How good or bad is that policy? How well will i perform if i follow it? For each state what will the value be not under optimal action but under this specific policy?
        - For a specific policy what the values are?

    - Fixed Policies
        - expectimax trees max over all actions to compute the optimal values
        - but if i have a fixed policy i don't have the choice between what action to take, so there's no max over a
        - if we fixed some policy π(s), then the tree would be simplier - only one action per state
            - ... though the tree's value would depend on which policy we fixed
        - so the algorithm to compute will be simplier and faster as well
    
    - Utilities for a Fixed Policy
        - basic operation: compute the utility of a state s under a fixed (generally non-optimal) policy
        - define the utility of a state s, under a fixed policy π:
            Vπ(s) = expected total discounted rewards starting in s and following and following π
        - Vπ(s) = Σ of all s' (T(s,π(s),s')[R(s,π(s),s') + λVπ(s')]) assuming acting not optimally but as π says in the future => Vπ(S')


Why do we evaluate policies?
    - sometimes we will have a policy and wanna know how good it is, just when running value iteration
    - we will see algorithms about starting with a π0 policy, evaluate that one, and look how to improve it


Policy Evaluation
    - How do we calculate the V's for a fixed policy π?
    - Compute Values from Actions
    - Idea 1: turn recursive Bellman equations into updates -> like value iteration
        - Vπ0(s) = 0
        - Vπk+1(s) = Σ of all s' (T(s,π(s),s')[R(s,π(s),s') + λVπk(s')])
        - complexity O(S^2) per iteration
    - Idea 2: without the maxes, the Bellman equations are just a linear system
        - just using any solver of linear equations -> Matlab?


Policy Extraction
    - what happens if we have the values and wanna know what policy we should use depending on them?
    - Computing Actions from Values