Markov decision processes II

Recap: MDPs
    - Markov decision processes:
        - states s
        - actions a
        - transitions P(s'| s,a) or T(s,a,s')
        - rewards R(s,a,s') (and discount Î»)
        - start state s0
    - Quantities:
        - Policy = map of states s to actions a
        - Utility = sum of discounted rewards
        - Values = expected future utility from a state (max node)
        - Q-Values = expected future utility from a q-state (chance node)

