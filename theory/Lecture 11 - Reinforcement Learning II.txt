Reinforcement Learning II

Reminder:
    - We stull assume an MDP:
        - a set of states s
        - a set of actions a (per state)
        - a model transition T(s,a,s')
        - a reward function R(s,a,s')

    - Still looking for a policy
    - New twist: don't know T or R, so must try out actions

    - Big Idea: Compute all averages over T using sample outcomes


Know MDP: Offline Solution
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Technique:
        - value / policy iteration
        - policy evaluation


Unknown MDP: Model-Based
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    Techninque:
        - VI / PI on approx. MDP
        - PE on approx. MDP


Unkown MDP: Model-Free
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Techinique:
        - Q-Learning
        - Value Learning


Model-Free Learning
    - literally, we are not computing the model and solved it  with respect to that same model
    - Model-free (temporal difference) learning:
        - experience world through episodes
            (s,a,r,s',a',r',s'',a'',r'',s''' ...)
        - update estimates each transition (s,a,r,s')
        - over time, updates will mimic Bellman updates

    - ** all model-free methods we are gonna talk update one transition at a time **
    - so every time we observe ourself going from some state s taking an action a and getting a reward r' in an outcome, thats a transition
    - and every transition will improve your knowledge and do an update


Q-Learning

    - We would like to do Q-Value updates to each Q-state:
        Qk+1(s,a) = Σ of all s' (T(s,a,s')[R(s,a,s') + λ · (maxQk(s',a') over all a')])
    - but can't compute this update without knowing T, R

    - Instead, so keep a running average
        Q(s,a) = (1 - α) · Q(s,a) + α · [R(s,a,s') + λ · (maxQk(s',a') over all a')])

    - ** every time will get these instantaneous one-step look-ahead samples we are roll it in into the running average weighted by alpha **
    - ** the bigger alpha is the more importance we place on new samples => we learn faster, but less stable because we give a lot of weight to each sample **


** Q-Learning is amazing because learns the optimal policy even though we don't follow it **
** but doesn't tells us how to select those actions **

Exploration vs Exploitation: trade-off
     - exploration: we try things, but knowing could be disastrous or not :)
     - exploitation: we continue do the things which currently appear to be good

How to Explore? -> how to select actions?
    - Simplest: random actions (ε-greedy)
        - every step, flip a coin
        - with (small) probability ε, act randomly
        - with (large) probability 1-ε, act on current policy
    - Problems with random actions?
        - you do eventually explore the sapace, but keep thrashing around once learning is done -> ** if u keep ε high **
        - the exploration is unstructured -> you try random things wheter is needed or not even if you know what the actions do
        - if you know what all the actions do you shouldn't necessarily be exploring anymore
            - One solution: lower ε over time
            - Another better solution: exploration functions

Exploration Functions -> When to explore?
    - Random actions: explore a fixed amount -> every state has an equal chance of handling an exploration action
    - Better idea: explore areas whose badness is not (yet) extablished, eventually stop exploring
        - in the face of uncertainty we should have optimism
        - but should last forever, so it might has a function like:
            - f(u, n) = u + k/n
            - takes a value estimate u and a visit count n, and returns an optimistic utility
            - rather than looking at just utilities of Q-states, we have a function that considers our guests at the utility and the number of times we've been there
            - n means the number of times we've tried that option out
            - so we take the utility and add an optimism bonus, that decreses as we visit the state more times n


Regret
    - even if we learn the optimal policy, we still make mistakes along the way
    - regret is a measure of our total mistake cost: the difference between our (expected) rewards, including youthful suboptimally, and optimal (expected) rewards
    - minimizing regret goes beyond learning to be optimal - it requires optimally learning to be optimal
    - example: random exploration and exploration functions both end up optimal, but random exploration has higher regret
    - minimizing regret is more like optimally learning to be optimal


** So, what we could do in a game like pac-man? **
    - there are so many states and we can't learn about each one
    - Idea: when we learn that one experience go scary, we should transfer that to all other similar states
    - when we learn ghost are scary we should transfer that to all other similar states

Approximate Q-Learning

    - Basic Q-Learning keeps a table of all q-values -> thats fine when there aren't too many q-states

    - In realistic situation, we cannot possibly learn about every single state!
        - too many states to visit them all in training
        - too many states to hold the q-tables in memory