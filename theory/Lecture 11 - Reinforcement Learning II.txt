Reinforcement Learning II

Reminder:
    - We stull assume an MDP:
        - a set of states s
        - a set of actions a (per state)
        - a model transition T(s,a,s')
        - a reward function R(s,a,s')

    - Still looking for a policy
    - New twist: don't know T or R, so must try out actions

    - Big Idea: Compute all averages over T using sample outcomes


Know MDP: Offline Solution
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Technique:
        - value / policy iteration
        - policy evaluation


Unknown MDP: Model-Based
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    Techninque:
        - VI / PI on approx. MDP
        - PE on approx. MDP


Unkown MDP: Model-Free
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Techinique:
        - Q-Learning
        - Value Learning


Model-Free Learning
    - literally, we are not computing the model and solved it  with respect to that same model
    - Model-free (temporal difference) learning:
        - experience world through episodes
            (s,a,r,s',a',r',s'',a'',r'',s''' ...)
        - update estimates each transition (s,a,r,s')
        - over time, updates will mimic Bellman updates