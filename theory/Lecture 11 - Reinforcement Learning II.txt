Reinforcement Learning II

Reminder:
    - We stull assume an MDP:
        - a set of states s
        - a set of actions a (per state)
        - a model transition T(s,a,s')
        - a reward function R(s,a,s')

    - Still looking for a policy
    - New twist: don't know T or R, so must try out actions

    - Big Idea: Compute all averages over T using sample outcomes


Know MDP: Offline Solution
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Technique:
        - value / policy iteration
        - policy evaluation


Unknown MDP: Model-Based
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    Techninque:
        - VI / PI on approx. MDP
        - PE on approx. MDP


Unkown MDP: Model-Free
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Techinique:
        - Q-Learning
        - Value Learning


Model-Free Learning
    - literally, we are not computing the model and solved it  with respect to that same model
    - Model-free (temporal difference) learning:
        - experience world through episodes
            (s,a,r,s',a',r',s'',a'',r'',s''' ...)
        - update estimates each transition (s,a,r,s')
        - over time, updates will mimic Bellman updates

    - ** all model-free methods we are gonna talk update one transition at a time **
    - so every time we observe ourself going from some state s taking an action a and getting a reward r' in an outcome, thats a transition
    - and every transition will improve your knowledge and do an update


Q-Learning

    - We would like to do Q-Value updates to each Q-state:
        Qk+1(s,a) = Σ of all s' (T(s,a,s')[R(s,a,s') + λ · (maxQk(s',a') over all a')])
    - but can't compute this update without knowing T, R

    - Instead, so keep a running average
        Q(s,a) = (1 - α) · Q(s,a) + α · [R(s,a,s') + λ · (maxQk(s',a') over all a')])