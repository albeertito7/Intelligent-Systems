Reinforcement Learning II

Reminder:
    - We stull assume an MDP:
        - a set of states s
        - a set of actions a (per state)
        - a model transition T(s,a,s')
        - a reward function R(s,a,s')

    - Still looking for a policy
    - New twist: don't know T or R, so must try out actions

    - Big Idea: Compute all averages over T using sample outcomes


Know MDP: Offline Solution
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Technique:
        - value / policy iteration
        - policy evaluation


Unknown MDP: Model-Based
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    Techninque:
        - VI / PI on approx. MDP
        - PE on approx. MDP


Unkown MDP: Model-Free
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Techinique:
        - Q-Learning
        - Value Learning


Model-Free Learning
    - literally, we are not computing the model and solved it  with respect to that same model
    - Model-free (temporal difference) learning:
        - experience world through episodes
            (s,a,r,s',a',r',s'',a'',r'',s''' ...)
        - update estimates each transition (s,a,r,s')
        - over time, updates will mimic Bellman updates

    - ** all model-free methods we are gonna talk update one transition at a time **
    - so every time we observe ourself going from some state s taking an action a and getting a reward r' in an outcome, thats a transition
    - and every transition will improve your knowledge and do an update


Q-Learning

    - We would like to do Q-Value updates to each Q-state:
        Qk+1(s,a) = Σ of all s' (T(s,a,s')[R(s,a,s') + λ · (maxQk(s',a') over all a')])
    - but can't compute this update without knowing T, R

    - Instead, so keep a running average
        Q(s,a) = (1 - α) · Q(s,a) + α · [R(s,a,s') + λ · (maxQk(s',a') over all a')])

    - ** every time will get these instantaneous one-step look-ahead samples we are roll it in into the running average weighted by alpha **
    - ** the bigger alpha is the more importance we place on new samples => we learn faster, but less stable because we give a lot of weight to each sample **


** Q-Learning is amazing because learns the optimal policy even though we don't follow it **
** but doesn't tells us how to select those actions **

Exploration vs Exploitation: trade-off
     - exploration: we try things, but knowing could be disastrous or not :)
     - exploitation: we continue do the things which currently appear to be good

How to Explore? -> how to select actions?
    - Simplest: random actions (ε-greedy)
        - every step, flip a coin
        - with (small) probability ε, act randomly
        - with (large) probability 1-ε, act on current policy
    - Problems with random actions?
        - you do eventually explore the sapace, but keep thrashing around once learning is done -> ** if u keep ε high **
        - the exploration is unstructured -> you try random things wheter is needed or not even if you know what the actions do
        - if you know what all the actions do you shouldn't necessarily be exploring anymore
            - One solution: lower ε over time
            - Another better solution: exploration functions

Exploration Functions -> When to explore?
    - Random actions: explore a fixed amount -> every state has an equal chance of handling an exploration action
    - Better idea: explore areas whose badness is not (yet) extablished, eventually stop exploring
        - in the face of uncertainty we should have optimism
        - but should last forever, so it might has a function like:
            - f(u, n) = u + k/n
            - takes a value estimate u and a visit count n, and returns an optimistic utility
            - rather than looking at just utilities of Q-states, we have a function that considers our guests at the utility and the number of times we've been there
            - n means the number of times we've tried that option out
            - so we take the utility and add an optimism bonus, that decreses as we visit the state more times n


Regret
    - even if we learn the optimal policy, we still make mistakes along the way
    - regret is a measure of our total mistake cost: the difference between our (expected) rewards, including youthful suboptimally, and optimal (expected) rewards
    - minimizing regret goes beyond learning to be optimal - it requires optimally learning to be optimal
    - example: random exploration and exploration functions both end up optimal, but random exploration has higher regret
    - minimizing regret is more like optimally learning to be optimal


** So, what we could do in a game like pac-man? **
    - there are so many states and we can't learn about each one
    - Idea: when we learn that one experience go scary, we should transfer that to all other similar states
    - when we learn ghost are scary we should transfer that to all other similar states

Approximate Q-Learning

    - Basic Q-Learning keeps a table of all q-values -> thats fine when there aren't too many q-states

    - In realistic situation, we cannot possibly learn about every single state!
        - too many states to visit them all in training
        - too many states to hold the q-tables in memory

    - Instead, we want to generalize:
        - learn about some small number of training states from experience
        - generalize that experience to new, similar situations
        - this is a fundamental idea in machine learning, and we'll see it over and over again

    - Solution: describe a state using a vector of features (properties)
        - features are functions from states to real numbers (often 0/1) that capture important properties of the state
        - example features:
            - distance to closes ghost
            - distance to closes dot
            - number of ghost
            - 1/(dist to dot)^2
            - Is Pacman in a tunnel? (0/!) ... etc
        - can also describe a q-state (s,a) with features
            - e.g. action moves closer to food


Linear Value Functions
    - using a feature representation, we can write a q function (or value function) for any state using a few weights -> linear decomposition
        V(s) = w1·f1(s) + w2·f2(s) + ... + wn·fn(s)
        Q(s,a) = w1·f1(s,a) + w2·f2(s,a) + ... + wn·fn(s,a)
    
    - Advantage: our experience is summed up in a few powerful numbers
    - Disadvantage: stats may share features but actually be very different in value!


Approximate Q-Learning
    - Q(s,a) = w1·f1(s,a) + w2·f2(s,a) + ... + wn·fn(s,a)
    - Q-learning with q-functions

    - transition = (s,a,r,s')
    - difference = [R(s,a,s') + λ · (maxQ(s',a') over all a')] - Q(s,a)  (difference = error between what you get and what you though were gonna get)
    - update Exact Q's: Q(s,a) = Q(s,a) + α·[difference]
    - Approximate Q's: wi = wi + α·[difference] · fi(s,a)
    - instead of increasing the Q-value directly we increase the weights
        - Intuitive interpretation:
            - adjust weights of active features
            - e.g. if something unexpectedly bad happens, blame the features that were on: disprefer all states with that state's features
    - our knowledge is encapsulated in a vector of feature weights, and we are evaluationg each q-state about how good or bad are its features and not by its q-state expected value

    ** you can have non-linear features but for now the final combination of them all must be linear **


    Q-Learning and Least Squares
    Linear Approximation: Regression*a
        - Prediction: ^y = w0 + w1 · f1(x)

        - Total Error: Σ (yi - ^yi)^2 over all i where yi is our observation, and the difference is our error
            - Σ ovser i of (yi - Σ over k of wk·fk(xi))^2


Minimizing Error*
    - imagine we had only one pont x, with features f(x), target value y, and weights w:
        - error(w) = (1/2)(y- Σ over k of wk·fk(x))^2
        -the error is the difference between the target y and the approximation ^y

        - derivating this expression to get the minimum, we now know that to make the error big we need to go to the direction of the feature vector that are contributing to this mistake
        - wm = wm + α·(y - Σ over k of wk·fk(x)) · fm(x)

        - Approximate q-update explained:
            - wm = wm + α·[R(s,a,s') + λ · (maxQ(s',a') over all a') - Q(s,a)] · fm(s,a)
            - where [R(s,a,s') + λ · (maxQ(s',a')] is the target, what you get
            - and Q(s,a) is the prediction, what you expect to get



Overfitting: Why Limiting Capacity Can Help*
    - using a linear combination of the features and not a non-linear
    - we want to generalize and approximate



Policy Search
    - you directly try to improve the policy

    - Problem: often the feature-based policies that work well (win games, maximize utilities) aren't the ones that approximate V/Q best

    - Solution: learn policies that maximize the rewards, not the values that predict them

    - Policy search: start with an ok solution (e.g. Q-learning) then fine-tune by hill climbing on feature weights

    - Simplest policy search:
        - start with an initial linear value function or Q-function
        - nudge each feature weight up and down and see if your policy is better than before
    
    - Problems:
        - How do we tell the policy got better?
        - Need to reun many sample episodes!
        - If there are a lot of features, this can be impractical!

    - Better methods exploit look-ahead structure, sample wisely, change multiple parameters ...