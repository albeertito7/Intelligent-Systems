Reinforcement Learning II

Reminder:
    - We stull assume an MDP:
        - a set of states s
        - a set of actions a (per state)
        - a model transition T(s,a,s')
        - a reward function R(s,a,s')

    - Still looking for a policy
    - New twist: don't know T or R, so must try out actions

    - Big Idea: Compute all averages over T using sample outcomes


Know MDP: Offline Solution
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Technique:
        - value / policy iteration
        - policy evaluation


Unknown MDP: Model-Based
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    Techninque:
        - VI / PI on approx. MDP
        - PE on approx. MDP


Unkown MDP: Model-Free
    - Goal:
        - compute V*, Q*, π*
        - evaluate a fixed policy π
    - Techinique:
        - Q-Learning
        - Value Learning


Model-Free Learning
    - literally, we are not computing the model and solved it  with respect to that same model
    - Model-free (temporal difference) learning:
        - experience world through episodes
            (s,a,r,s',a',r',s'',a'',r'',s''' ...)
        - update estimates each transition (s,a,r,s')
        - over time, updates will mimic Bellman updates

    - ** all model-free methods we are gonna talk update one transition at a time **
    - so every time we observe ourself going from some state s taking an action a and getting a reward r' in an outcome, thats a transition
    - and every transition will improve your knowledge and do an update


Q-Learning

    - We would like to do Q-Value updates to each Q-state:
        Qk+1(s,a) = Σ of all s' (T(s,a,s')[R(s,a,s') + λ · (maxQk(s',a') over all a')])
    - but can't compute this update without knowing T, R

    - Instead, so keep a running average
        Q(s,a) = (1 - α) · Q(s,a) + α · [R(s,a,s') + λ · (maxQk(s',a') over all a')])

    - ** every time will get these instantaneous one-step look-ahead samples we are roll it in into the running average weighted by alpha **
    - ** the bigger alpha is the more importance we place on new samples => we learn faster, but less stable because we give a lot of weight to each sample **


** Q-Learning is amazing because learns the optimal policy even though we don't follow it **
** but doesn't tells us how to select those actions **

Exploration vs Exploitation: trade-off
     - exploration: we try things, but knowing could be disastrous or not :)
     - exploitation: we continue do the things which currently appear to be good

How to Explore? -> how to select actions?
    - Simplest: random actions (ε-greedy)
        - every step, flip a coin
        - with (small) probability ε, act randomly
        - with (large) probability 1-ε, act on current policy
    - Problems with random actions?
        - you do eventually explore the sapace, but keep thrashing around once learning is done -> ** if u keep ε high **
        - the exploration is unstructured -> you try random things wheter is needed or not even if you know what the actions do
        - if you know what all the actions do you shouldn't necessarily be exploring anymore
            - One solution: lower ε over time
            - Another better solution: exploration functions

Exploration Functions -> When to explore?
    - Random actions: explore a fixed amount -> every state has an equal chance of handling an exploration action
    - Better idea: explore areas whose badness is not (yet) extablished, eventually stop exploring
        - in the face of uncertainty we should have optimism
        - but should last forever, so it might has a function like
            f(u, n) = u + k/n